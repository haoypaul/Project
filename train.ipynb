{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ec09c05-eb1e-4365-9268-f83d79e19714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 35.25%\n",
      "Epoch: 1 \tTraining Loss: 2.049681 \tValidation Loss: 1.799485\n",
      "Validation loss decreased (inf --> 1.799485).  Saving model ...\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 41.61%\n",
      "Epoch: 2 \tTraining Loss: 1.558186 \tValidation Loss: 1.678528\n",
      "Validation loss decreased (1.799485 --> 1.678528).  Saving model ...\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.88%\n",
      "Epoch: 3 \tTraining Loss: 1.348117 \tValidation Loss: 1.512662\n",
      "Validation loss decreased (1.678528 --> 1.512662).  Saving model ...\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.54%\n",
      "Epoch: 4 \tTraining Loss: 1.183386 \tValidation Loss: 1.313750\n",
      "Validation loss decreased (1.512662 --> 1.313750).  Saving model ...\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.88%\n",
      "Epoch: 5 \tTraining Loss: 1.074062 \tValidation Loss: 1.238933\n",
      "Validation loss decreased (1.313750 --> 1.238933).  Saving model ...\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.71%\n",
      "Epoch: 6 \tTraining Loss: 1.000067 \tValidation Loss: 0.872880\n",
      "Validation loss decreased (1.238933 --> 0.872880).  Saving model ...\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.58%\n",
      "Epoch: 7 \tTraining Loss: 0.924699 \tValidation Loss: 0.907587\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.26%\n",
      "Epoch: 8 \tTraining Loss: 0.855978 \tValidation Loss: 0.831369\n",
      "Validation loss decreased (0.872880 --> 0.831369).  Saving model ...\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.85%\n",
      "Epoch: 9 \tTraining Loss: 0.797777 \tValidation Loss: 0.792299\n",
      "Validation loss decreased (0.831369 --> 0.792299).  Saving model ...\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.53%\n",
      "Epoch: 10 \tTraining Loss: 0.762155 \tValidation Loss: 0.744260\n",
      "Validation loss decreased (0.792299 --> 0.744260).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.readData import read_dataset\n",
    "from utils.ResNet import ResNet18\n",
    "\n",
    "# set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# 读数据\n",
    "batch_size = 128\n",
    "train_loader, valid_loader, test_loader = read_dataset(batch_size=batch_size, pic_path='dataset')\n",
    "# 加载模型(使用预处理模型，修改最后一层，固定之前的权重)\n",
    "n_class = 10\n",
    "model = ResNet18()\n",
    "\"\"\"\n",
    "ResNet18网络的7x7降采样卷积和池化操作容易丢失一部分信息,\n",
    "所以在实验中我们将7x7的降采样层和最大池化层去掉,替换为一个3x3的降采样卷积,\n",
    "同时减小该卷积层的步长和填充大小\n",
    "\"\"\"\n",
    "model.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.fc = torch.nn.Linear(512, n_class)  # 将最后的全连接层改掉\n",
    "model = model.to(device)\n",
    "# 使用交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 开始训练\n",
    "n_epochs = 10\n",
    "valid_loss_min = np.Inf  # track change in validation loss\n",
    "accuracy = []\n",
    "lr = 0.1\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f'Epoch {epoch}/{n_epochs}')\n",
    "    \n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    total_sample = 0\n",
    "    right_sample = 0\n",
    "    \n",
    "    # 动态调整学习率\n",
    "    if counter / 10 == 1:\n",
    "        counter = 0\n",
    "        lr = lr * 0.5\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    ###################\n",
    "    # 训练集的模型 #\n",
    "    ###################\n",
    "    model.train()  # 启用batch normalization和drop out\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for data, target in train_loader_tqdm:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # 清除梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 正向传递：计算预测输出\n",
    "        output = model(data).to(device)\n",
    "        # 计算损失值\n",
    "        loss = criterion(output, target)\n",
    "        # 反向传递：计算损失相对于模型参数的梯度\n",
    "        loss.backward()\n",
    "        # 执行单个优化步骤（参数更新）\n",
    "        optimizer.step()\n",
    "        # 更新训练损失\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # 验证集的模型 #\n",
    "    ######################\n",
    "    model.eval()  # 验证模型\n",
    "    valid_loader_tqdm = tqdm(valid_loader, desc=\"Validation\", leave=False)\n",
    "    for data, target in valid_loader_tqdm:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # 正向传递：计算预测输出\n",
    "        output = model(data).to(device)\n",
    "        # 计算损失值\n",
    "        loss = criterion(output, target)\n",
    "        # 更新验证损失\n",
    "        valid_loss += loss.item() * data.size(0)\n",
    "        # 将输出概率转换为预测类\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # 将预测与真实标签进行比较\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        total_sample += data.size(0)\n",
    "        right_sample += correct_tensor.sum().item()\n",
    "    \n",
    "    # 计算准确率\n",
    "    accuracy_epoch = 100 * right_sample / total_sample\n",
    "    print(f\"Accuracy: {accuracy_epoch}%\")\n",
    "    accuracy.append(accuracy_epoch)\n",
    " \n",
    "    # 计算平均损失\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "    \n",
    "    # 显示训练集与验证集的损失函数\n",
    "    print(f'Epoch: {epoch} \\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {valid_loss:.6f}')\n",
    "    \n",
    "    # 如果验证集损失函数减少，就保存模型。\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print(f'Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint/resnet18_cifar10.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce1d5ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.16%\n",
      "Epoch: 1 \tTraining Loss: 1.742501 \tValidation Loss: 1.644033\n",
      "Validation loss decreased (inf --> 1.644033). Saving model ...\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.05%\n",
      "Epoch: 2 \tTraining Loss: 1.392996 \tValidation Loss: 1.513441\n",
      "Validation loss decreased (1.644033 --> 1.513441). Saving model ...\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.84%\n",
      "Epoch: 3 \tTraining Loss: 1.196607 \tValidation Loss: 1.120430\n",
      "Validation loss decreased (1.513441 --> 1.120430). Saving model ...\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.19%\n",
      "Epoch: 4 \tTraining Loss: 1.066062 \tValidation Loss: 0.970725\n",
      "Validation loss decreased (1.120430 --> 0.970725). Saving model ...\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.26%\n",
      "Epoch: 5 \tTraining Loss: 0.960182 \tValidation Loss: 1.030067\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.5%\n",
      "Epoch: 6 \tTraining Loss: 0.872636 \tValidation Loss: 0.786156\n",
      "Validation loss decreased (0.970725 --> 0.786156). Saving model ...\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.01%\n",
      "Epoch: 7 \tTraining Loss: 0.814261 \tValidation Loss: 0.676117\n",
      "Validation loss decreased (0.786156 --> 0.676117). Saving model ...\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.91%\n",
      "Epoch: 8 \tTraining Loss: 0.763166 \tValidation Loss: 0.702353\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.94%\n",
      "Epoch: 9 \tTraining Loss: 0.715948 \tValidation Loss: 0.615700\n",
      "Validation loss decreased (0.676117 --> 0.615700). Saving model ...\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19908\\2421495454.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('checkpoint/resnet18_cifar10.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.39%\n",
      "Epoch: 10 \tTraining Loss: 0.677611 \tValidation Loss: 0.591619\n",
      "Validation loss decreased (0.615700 --> 0.591619). Saving model ...\n",
      "Model saved as binary file: 'checkpoint/resnet18_cifar10_int8.bin'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.readData import read_dataset\n",
    "from utils.ResNet import ResNet18\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_loader, valid_loader, test_loader = read_dataset(batch_size=batch_size, pic_path='dataset')\n",
    "\n",
    "# 加载模型\n",
    "n_class = 10\n",
    "model = ResNet18()\n",
    "model.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.fc = torch.nn.Linear(512, n_class)  # 修改最后的全连接层\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "lr = 0.01 \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# 训练\n",
    "n_epochs = 10\n",
    "valid_loss_min = float(\"inf\") \n",
    "accuracy = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f'Epoch {epoch}/{n_epochs}')\n",
    "    \n",
    "    # 训练和验证损失\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    total_sample = 0\n",
    "    right_sample = 0\n",
    "\n",
    "    ###################\n",
    "    # 训练模型 #\n",
    "    ###################\n",
    "    model.train()\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for data, target in train_loader_tqdm:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # 清除梯度\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).to(device)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 更新训练损失\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # 验证模型 #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    valid_loader_tqdm = tqdm(valid_loader, desc=\"Validation\", leave=False)\n",
    "    for data, target in valid_loader_tqdm:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # 正向传播\n",
    "        output = model(data).to(device)\n",
    "        \n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item() * data.size(0)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        \n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        total_sample += data.size(0)\n",
    "        right_sample += correct_tensor.sum().item()\n",
    "    \n",
    "    # 计算acc\n",
    "    accuracy_epoch = 100 * right_sample / total_sample\n",
    "    print(f\"Accuracy: {accuracy_epoch}%\")\n",
    "    accuracy.append(accuracy_epoch)\n",
    " \n",
    "\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "\n",
    "    print(f'Epoch: {epoch} \\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {valid_loss:.6f}')\n",
    "    \n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print(f'Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint/resnet18_cifar10.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoint/resnet18_cifar10.pt'))\n",
    "\n",
    "# 量化模型\n",
    "model.eval()\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model.cpu(),  \n",
    "    {nn.Linear},  # 量化全连接层\n",
    "    dtype=torch.qint8  # 量化为8位 int8\n",
    ")\n",
    "\n",
    "# 保存\n",
    "torch.save(model_int8.state_dict(), 'checkpoint/resnet18_cifar10_int8.pt')\n",
    "\n",
    "\n",
    "with open('checkpoint/resnet18_cifar10_int8.bin', 'wb') as f:\n",
    "    for param_tensor in model_int8.state_dict().values():\n",
    "        if isinstance(param_tensor, torch.Tensor):\n",
    "            f.write(param_tensor.cpu().numpy().tobytes())\n",
    "\n",
    "print(\"Model saved as binary file: 'checkpoint/resnet18_cifar10_int8.bin'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('dataNormal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "a83c3f418a14762567abd69e6ddd05526675949ea032a3796e0043cb24f0a0f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
